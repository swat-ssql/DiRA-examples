[["index.html", "Directional Regression Analysis &amp; the DiRA Package Chapter 1 Overview", " Directional Regression Analysis &amp; the DiRA Package Ella Foster-Molina 2021-08-14 Chapter 1 Overview table, td, th { border: none; padding-left: 1em; padding-right: 1em; margin-left: auto; margin-right: auto; margin-top: 1em; margin-bottom: 1em; } Draft version, not for distribution. Please do not share this link. This document overviews some ways to use the Directional Regression Analysis package. Consider a multiple linear regression surface defined by \\[\\begin{align*} \\hat{y} = \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + ... + \\hat{\\beta}_n x_n + \\hat{\\beta}_0. \\end{align*}\\] Many commonly examined nested models within that multiple regression are simply directions in the regression surface. These directions can be usefully analyzed by creating a new axis system that is a rotation of the original axis system. These are the core commands for performing these analyses. Additional details about options available for these commands is available at (put link here). stargazer.dira(formula, data, model.names, data.subsets) Takes a formula, dataset, a vector of model names, and any subsets of the data needed. Additional options are available. Outputs a pretty summary of the rotated and nested models defined in model.names. The first two explanatory variables in the formula are used to create the nests and rotations. models.dira(formula, data, model.names, data.subsets) Takes a formula, dataset, a vector of model names, and any subsets of the data needed. Additional options are available. Creates a named list of rotated and nested linear models. You can examine the models within that list in more depth using tools like glance.dira() and tidy.dira(), or extract individual models to evaluate separately. glance.dira(dira_models) Takes a named list of models created by models.dira() Outputs a tidy overview of each modelâ€™s statistics tidy.dira(dira_models) Takes a named list of models created by models.dira() Outputs a tidy overview of the coefficients in each model add_3d.directions(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. Outputs an interactive 3D plot showing the regression surface as well as all specified nested and rotated models. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within add_3d.directions. The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. add_2d.directions(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. Outputs a graphic with 2-4 2D subplots showing the nested models and rotated models projected into 2D plots. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within add_2d.directions(). The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. Note: The graphics object generated by add_2d.directions cannot be modified by the layout command, although plotly object inputted can include a layout layer. The annotate() command can add annotations to the graphics object. Not all models can be plotted in 2D at the moment. plotlist.2d(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within plotlist.2d(). The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. Not all models can be plotted in 2D at the moment. Outputs a list of 2D graphics showing crosssections of the 3D graphic created by add_3d.directions. You can extract individual plots to use as needed. It is currently possible to examine the following nested and rotated models of the multiple linear regression surface. It is also possible to show directional symmetries between the nested and original models for categorical explanatory variables by applying a nested model the original data subsetted to only include one or two of the categories. model model name direction of first axis direction of second axis line color(s) y ~ x1 + x2 lm_multiple x1 x2 red &amp; orange y ~ x1 lm_y.predby.x1 x2 predicted by x1 light blue y ~ x1 + x2 residuals lm_y.predby.x1.w.resids x2 predicted by x1 residuals of x2 dark blue &amp; blue y ~ x2 lm_y.predby.x2 x1 predicted by x2 fuchsia y ~ x2 + x1 residuals lm_y.predby.x2.w.resids x1 predicted by x2 residuals of x1 purple &amp; pink y ~ PC1 lm_pc first principal component light grey y ~ PC1 + PC2 lm_pc.w.orth first principal component orthogonal to first axis black &amp; grey y ~ new.dir + orth lm_new.direction user defined direction orthogonal to first axis green &amp; lime green "],["MultipleRegression.html", "Chapter 2 Multiple Regression: County level voting behavior in 2016 2.1 Regression results for the original and nested models 2.2 Regression visualization in 3D 2.3 Examining the nested model in 2D", " Chapter 2 Multiple Regression: County level voting behavior in 2016 This example introduces you to the package by demonstrating a regression surface in 3D with marginal effects shown. It will then connect the regression surface to the more familiar scatterplot with a regression line. The data shows county level demographics as they relate to the percent of the county that voted for Trump in 2016. This example will focus on the percent of the county that has completed some higher education, regardless of whether they completed a higher education degree and the percent of the county that is unemployed. Consider the following estimated regression line, \\[\\begin{align*} \\hat{y} &amp;= \\hat{\\beta}_1 x_1 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} &amp;= -0.534 * \\text{% with higher education} + 90.913\\\\ \\end{align*}\\] The corresponding scatterplot and regression line are visualized below. There is a clear negative relationship between the two variables. Increasing higher education experience in a county is associated with lower support for Trump in the 2016 election. However, this only examines one possible relationship in the data. Unemployment would also be expected to have a relationship with the Trump vote. A multiple regression, which predicts a regression surface, can examine how both county level unemployment and education relate to support for Trump. This regression takes the form \\[\\begin{align*} \\hat{y} &amp;= \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} = &amp; -0.781 * \\text{% with higher education} - \\\\ &amp; 16.727 * \\text{logged % unemployed} + 130.274. \\end{align*}\\] 2.1 Regression results for the original and nested models The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression results associated with the original multiple linear regression. The regression results for the nested model \\[\\text{estimated % Trump vote} = -0.534 * \\text{% with higher education} + 90.913\\] are in Model 2. % Trump vote in 2016 original model nested model (1) (2) % with higher education -0.781*** -0.534*** (0.026) (0.024) logged % unemployed -16.727*** (0.817) Constant 130.274*** 90.913*** (2.265) (1.277) Observations 3,139 3,139 Adjusted R2 0.234 0.132 F Statistic 481.583*** (df = 2; 3136) 479.755*** (df = 1; 3137) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 2.2 Regression visualization in 3D The image below is created using the add_3d.directions() command in the DiRA package. It shows: A scatterplot of counties measured by the percent of their population with higher education, the logged percent unemployment in each county, and the percent of the county that voted for Trump in 2016. The regression surface defined by Model 1. The marginal effect of each of the variables within Model 1 and Model 2. Confidence intervals associated with the regression surface and the marginal effects of the variables in Model 1 and Model 2. To interact with the 3D visual: Hover over points to see the county name and state. Click and drag on the graphic to rotate it. Click on items in the legend to remove specific parts of the image; click on that item again to put it back. To reset the image to its starting point, hover over the image and click on the house symbol (or reload the page). Model 1, \\(y \\sim x_1 + x_2\\), is shown with regression surface and a confidence interval surface. The marginal effects are shown in red and orange lines. Hover over and spin the graphic to examine the effects described below. Since this section focuses on Model 1, you may want to remove the dashed light blue line representing Model 2 for the moment. The marginal effect of \\(x_1\\) (% with higher education) in red, shows how much y (% Trump) changes when \\(x_2\\) (log % unemployed) is held constant. In this model, the effect of % with higher education is strongly negative. Holding unemployment constant, counties with low education levels (~30% with higher education) tend to have over 80% of their voters vote for Trump. Those with high education levels (~80% with higher education) tend to have less than 40% of their voters vote for Trump. The marginal effect of \\(x_2\\) (log % unemployed) in orange, shows how much y (% Trump) changes when \\(x_1\\) (% with higher education) is held constant. In this model, the effect of % with higher education is also strongly negative holding education levels constant. That is, counties with higher unemployment tend to have lower rates of voting for Trump in 2016 when county education levels are controlled for. In this visualization, the marginal effect of \\(x_1\\) (\\(x_2\\)) is shown holding \\(x_2\\) (\\(x_1\\)) at its mean. The DiRA package defaults to holding all other covariates at their means, but any value could be chosen. The y-intercept for the marginal effect would change, but the slope would remain the same regardless of the value chosen. 2.3 Examining the nested model in 2D The image above is created using the add_2d.directions() command in the DiRA package. Model 2, \\(y \\sim x_1\\) is a nested model of Model 1 and only has one axis. This axis is shown with a light blue dashed line, and it has a direction in the \\(x_1\\), \\(x_2\\) coordinate system defined by the best fit line for \\(x_2 \\sim x_1\\), \\[\\text{logged % unemployed} = -0.534 * \\text{% with higher education} + 90.913.\\] You can (and should!) spin the 3D graphic to reflect the 2D images. The images are not identical because the 3D image has a perspective shift, but you can see that the 2D images are simply crossections of the 3D graphic. Tips to improve the view: Focus on the light blue dashed line that represents Model 2 in the 3D graphic by removing the red and orange lines (click on the corresponding items in the legend). Click and drag from the top of the 3D plot to the bottom to see the top of the regression surface. Hover over individual scatterpoints in any of these images to see which state, district, and year the elections occurred in, and match the points in the 3D and 2D plots by the specific election they represent. "],["interaction-terms-county-level-voting-behavior-in-2016.html", "Chapter 3 Interaction Terms: County level voting behavior in 2016 3.1 Regression results for a model with interaction terms 3.2 3D changing marginal effects 3.3 2D marginal effects", " Chapter 3 Interaction Terms: County level voting behavior in 2016 This example highlights a regression surface estimated from a model with an interaction term. The regression examines the relationship between: Outcome variable: The percent of the countyâ€™s voters that voted for Donald Trump in 2016. Explanatory variable: The logged opioid overdosed death rate in 2014, where the original opioid overdose death rate is measured in deaths per 100,000 people. Explanatory variable: A binary variable that is true/1 if a majority of the adults in the county had attended college in 2016, whether or not they attained a degree. It is false/0 otherwise. Interaction terms in multiple regressions introduce an element of nonlinearity to the regression surface, although the marginal effects of each variable retain a linear relationship with the outcome variable. The following description highlights both the nonlinear, linearity, and relationship between the regression surface and regression lines predicted by \\(y \\sim x_1\\) for subsets of the data that isolate to one of the binary values for the college attendance variable. The second explanatory variable is binary for ease of interpretation and to show the relationship between the regression surface and the nested models applied to subsets of the data. 3.1 Regression results for a model with interaction terms Consider the following estimated regression line, \\[\\begin{align*} \\hat{y} =&amp; \\hat{\\beta}_1 x_1 +\\hat{\\beta}_2 x_2 +\\hat{\\beta}_{12} x_1 x_2 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} =&amp; - 6.652 * \\text{opioid death rate (log)} + \\\\ &amp; 19.832 * \\text{majority attended college} \\\\ &amp; -13.056 * \\text{opioid death rate (log)}* \\text{majority attended college}+ 52.753 \\\\ \\end{align*}\\] The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression results associated with the original multiple linear regression. The regression results for the nested model using only the subset of the data with low opioid overdose deaths (\\(x_2 = 0\\)) are in Model 2, and the results where opioid overdose deaths are high are in Model 3. All coefficients are statistically significant. However, the original model is difficult to interpret directly. It is easy but erroneous to interpret the logged opioid death rate coefficient of 6.652 in Model 1 as positive regardless of the value of the college attendance variable. However, the logged opioid death rate coefficient is only positive and 6.652 when the \\(x_2\\) variable is zero, which is when the majority of the county did not attend college. When the majority of the county does attend college, the marginal effect of the logged opioid death rate becomes strongly negative. This is evident in the results from Models 2 and 3, where the logged opioid death rate flips direction between low and high education counties. The interaction term shows how much the marginal effect of \\(x_1\\) changes as \\(x_2\\) changes, and vice versa. It also shows whether that change is statistically significant. In this case it is statistically significant. That is, we believe that the difference in the slopes of the \\(x_1\\) as \\(x_2\\) changes are not due to chance. This is one of the key benefits of using an interaction term instead of running a nested regression on a subset of the data. As youâ€™ll see in the 3D visualization, the marginal effect of the logged opioid death rate in the original model when \\(x_2 = 0\\) is identical to the coefficient on the logged opioid death rate in Model 2. Youâ€™ll also see that the standard errors and confidence intervals are slightly different. % Trump vote in 2016 original model nested: subset x2=0 nested: subset x2=1 (1) (2) (3) opioid death rate (log) 6.652*** 6.652*** -6.404*** (0.714) (0.652) (0.711) majority attended college 19.832*** (2.179) opioid death rate (log):majority attended college -13.056*** (0.975) Constant 52.753*** 52.753*** 72.585*** (1.681) (1.535) (1.482) Observations 3,139 1,456 1,683 Adjusted R2 0.120 0.066 0.046 F Statistic 144.219*** (df = 3; 3135) 104.008*** (df = 1; 1454) 81.209*** (df = 1; 1681) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 3.2 3D changing marginal effects The visual below shows the marginal effect of the logged rate of opioid overdose deaths (\\(x_1\\)) with two red lines. One line shows the marginal effect for counties where a majority attended college and the other for counties where a majority did not attend college. Unlike the regression example above without an interaction effect, the marginal effects have very different slopes based on college experience. The nested models are shown with pink dashed lines. The predicted values of the nested and original models are identical for counties with high education or low education, so the pink dashed line lies on top of the red line. The confidence intervals, however, are slightly different between the marginal effects in the nested and original models. If you zoom in far enough, youâ€™d see that the confidence interval on the logged opioid death rate is slightly wider for high education counties. It is slightly narrower for counties with low rates of college attendance. To see how much the marginal effect of one variable change based on the value of another in your own data, you can: Use the DiRA package (currently available by asking the author, Ella Foster-Molina, for a copy). Run the regression \\(y \\sim x_1\\) on different values or subsets of \\(x_2\\) (or vice versa). Calculate the marginal effect of \\(x_1\\) from the estimated regression surface equation, plugging in new values of \\(x_2\\) (or vice versa). 3.3 2D marginal effects One benefit of being able to visualize the marginal effects and confidence intervals of the original model for different values of the college attendance variable is that you can see whether the marginal effect itself is statistically significantly positive/negative for any given value of college attendance. There is undoubtedly a way to calculate the statistical significance of the marginal effect of \\(x_1\\) for a given value of \\(x_2\\), but it would be complex (is it? I donâ€™t know how to do it off the top of my head). Each of the marginal effects plotted above is statistically significantly different from zero. "],["rotated-nested-models-black-voter-turnout-data.html", "Chapter 4 Rotated &amp; Nested Models: Black voter turnout data 4.1 Directional regression results 4.2 3D regression visualization 4.3 2D nested and rotated models", " Chapter 4 Rotated &amp; Nested Models: Black voter turnout data This dataset provides a classic example where the sign and statistical significance of one of the coefficients, whether a Black candidate runs in an election, flips between the full regression model and the nested model. The DiRA package provides a method to analyze and visualize the joint change reflected by the nested model that creates this sign flip. You will also be able to examine the rotated axes within the full regression model that produce the same coefficient as in the nested model for the effect of having a Black candidate. Each row in this dataset represents an election in a Congressional district between 2006 and 2010. The regression will examine the percentage of the Black population that votes based on whether there is a Black candidate in the election and the percentage of the district that is Black. The regression results for this model are created using the stargazer.dira() command in the DiRA package. 4.1 Directional regression results % Black turnout original model nested model rotated model (1) (2) (3) Black candidate -0.736 6.164*** 6.164*** (2.170) (1.498) (1.488) % Black voters 0.207*** (0.047) residuals of % Black voters 0.207*** (0.047) Constant 37.528*** 39.386*** 39.386*** (0.668) (0.518) (0.515) Observations 1,237 1,237 1,237 Adjusted R2 0.027 0.013 0.027 F Statistic 18.118*** (df = 2; 1234) 16.924*** (df = 1; 1235) 18.118*** (df = 2; 1234) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Model 1 shows that the estimated regression surface can be defined by \\[\\begin{align*} \\hat{y} &amp;= \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_0\\\\ \\text{estimated % Black turnout} = &amp; -0.736 * \\text{Black candidate}+ \\\\ &amp;0.207 * \\text{% Black voters} + 37.528. \\end{align*}\\] There is a nonsignificant and negative relationship between having a Black candidate in an election and Black turnout, when the percent of the voters who are Black is held constant. On the other hand, there is a strong positive and significant relationship between the percent of a voters who are Black and the percent of the Black population that turns out, when having a Black candidate is held constant. Model 2 creates a nested model by omitting % Black voters. The resulting regression is \\[\\begin{align*} \\text{estimated % Black turnout} = 6.164 * \\text{Black candidate}+ 39.386. \\end{align*}\\] It shows a strong positive relationship between the Black turnout percentage and whether the candidate is Black. Specifically, a district with a Black candidate generally has a 6.16% higher turnout among the Black voters. You will see that this is largely because most districts where a Black candidate runs have a large Black voting population, and large Black communities tend to have a high turnout rate. This creates a joint change where both the % of a district that is Black and having a Black candidate changes simulatenously. The 3D graphic below visualize this joint change. Model 3 is a rotated model that retains the joint change information from the nested model in Model 2, but adds back a variable that contains part of the information of the Black voting population. Specifically, it includes a variable that is the residuals of the regression. This defines a rotated version of the same regression surface defined by Model 1. \\[\\begin{align*} \\text{estimated % Black turnout} = &amp;6.164* \\text{Black candidate}+ \\\\ &amp; 0.207 *\\text{residuals of % Black voters} + 39.386 \\end{align*}\\] This results in a rotation of the original axes defined by the relationship between the explanatory variables. Since all information about \\(x_1\\) and \\(x_2\\) from the original model is retained, The regression surfaces and confidence intervals defined by Models 1 and 3 are identical except for the rotation. All rotated and nested models can be interpreted in the coordinate system used by the original model, which facilitates comparisons and interpretations. Therefore the DiRA package plots all rotated and nested models in the coordinate system used by the original model. The identical regression surfaces in Models 1 and 3 are reflected by the identical F-statistic and adjusted R squared values for Models 1 and 3. Any rotated model like Model 3 has more information than a nested model like Model 2, so it is usually more precise. This is the case here, where the standard error on Model 3â€™s coefficient for Black candidate is slightly smaller than Model 2â€™s. 4.2 3D regression visualization The image is created using the add_3d.directions() command in the DiRA package. To interact with this visual: Hover over points to see the district and year a given election occurred. Click and drag on the graphic to rotate it. Click on items in the legend to remove specific parts of the image; click on that item again to put it back. To reset the image to its starting point, hover over the image and click on the house symbol at the top right. It shows the marginal effect of each of the variables within the models above. Model 1, \\(y \\sim x_1 + x_2\\), is shown with regression surface and a confidence interval surface. The marginal effects are shown in red and orange lines. Model 2, \\(y \\sim x_1\\), shows a nested model, which only has one axis, shown in light blue dashed line. Model 3, \\(y \\sim x_1 + x_2 \\text{residuals}\\), uses the same information as Model 1, but rotates the axis system. The axes are represented by the dark blue and blue lines. Note that the direction of the \\(x_1\\) axis is identical to the \\(x_1\\) axis of Model 2, and therefore appears under the dashed light blue line representing the marginal effect of \\(x_1\\) in Model 2. The direction of \\(x_2\\)â€™s residuals are identical to the direction of \\(x_2\\) in the original model, and appear under the orange marginal effect for the original model. Note that the DiRA package always uses the axis system defined by the original regression models. Although the axes for \\(x_1\\) and \\(x_2\\text{residuals}\\) are not orthogonal to each other when plotted in the \\(x_1\\), \\(x_2\\) axis system, the observations in the data are orthogonal to each other when the \\(x_1\\), \\(x_2\\text{residuals}\\) axis system is used. 4.3 2D nested and rotated models The 2D images are created using the add_2d.directions() command in the DiRA package. They focus on the nested model from Model 2 and the rotated model from Model 3. You can (and should!) spin the 3D graphic to reflect the 2D images. The images are not identical because the 3D image has a perspective shift, but you can see that the 2D images are simply crossections of the 3D graphic. Tips to improve the view: Focus on the blue lines in the 3D graphic by removing the red and orange lines (click on the corresponding items in the legend). Click and drag from the top of the 3D plot to the bottom to see the top of the regression surface. Hover over individual scatterpoints in any of these images to see which state, district, and year the elections occured in, and match the points in the 3D and 2D plots by the specific election they represent. The right hand 2D image shows the best fit line between the percent of the voters who are Black and whether a Black candidate is in the election. The equation for this line is \\[\\begin{align*} \\text{% Black voters} = 33.27* \\text{Black candidate} + 8.96. \\end{align*}\\] An election with a Black candidate has, on average, 33.27% more Black voters than an election without a Black candidate. You now have enough information to fully interpret the joint direction shown in by the dark blue and light blue lines. Specifically, the joint effect of moving FROM an election without a Black candidate TO an election with a Black candidate that also has 33.27% more Black voters results in a 6.16% increase in Black turnout. This joint change in \\(x_1\\) and \\(x_2\\) is associated with a statistically significant change in \\(y\\) in both the rotated and nested models. For any nested model \\(y \\sim x_1\\) of \\(y \\sim x_1 + x_2\\), a one unit change in \\(x_1\\) is associated with a change in \\(x_2\\) defined by the best fit line \\(x_2 \\sim x_1\\), and they jointly produce a change in \\(y\\) defined by \\(y \\sim x_1\\). "],["categorical-data-rotated-models-simulated-hair-data.html", "Chapter 5 Categorical Data &amp; Rotated Models: Simulated hair data 5.1 Directional regression results 5.2 3D rotated model 5.3 2D visuals", " Chapter 5 Categorical Data &amp; Rotated Models: Simulated hair data This dataset on hair lengths demonstrates how directional regression analysis can provide useful insights into regressions with categorical explanatory variables. It is a simulated dataset that shows a relationship between gender identification and hair length. The gender categories used in this simulation are male, female, and nonbinary. 5.1 Directional regression results To understand the rotations and subsets that are created in the models, itâ€™s first important to understand how categorical variables are represented in a regression. Each category within the categorical variable represents its own axis. Each categoryâ€™s axis has FALSE/0 represent those observations that are not part of that category, and TRUE/1 represent those observations that are part of the category. For example, the female axis has 1 indicate females, and 0 indicate not females. Programs typically code 0 as FALSE, and 1 as TRUE. However, every new axis must provide new information in the model. You can see in the left hand image below that the coordinate system that uses female and male as the axes uniquely identify the nonbinary folk at the origin of the plot; they are the only group that is both not female and not male, because there are only 3 categories available. The rotation of these axes on the right hand side uses nonbinary and female as the axes, and equally shows that the male category is uniquely represented at the origin. An additional axis to represent the category that is at the origin would not provide new information, and therefore is not allowed in a regression. Some programs (like R) will simply drop redundant variables if you try to include them in a regression, while others will throw errors. The regression results for this model are created using the stargazer.dira() command in the DiRA package. The results below show the rotations and nests of the multiple regression based on the left hand axis system in the plots above, where the x1-axis represents females and the x2-axis represents males. Model 1 shows the regression \\(length = 6.042 * female - 2.658* male + 4.589\\). The reference category is nonbinary, so females tend to have hair lengths 6.042 inches longer than nonbinary folk. Men tend to have hair lengths 2.658 inches shorter than nonbinary folk. Model 2 shows the rotation of the female and male axes that represents the direct comparison between males and females. This is done by telling stargazer.dira() to show the results for a rotation of the original axes where the primary axis moves from the point (not female, male) = (0, 1) to the point (female, not male) = (1,0). This rotation shows that females tend to have hair lengths 8.7 inches longer than men. There is also an orthogonal component included; without it, the male vs female direction would be a joint direction from male to a combination of female and male instead of the difference between males and female holding the nonbinary variable constant. The orthogonal component has no meaningful interpretation on its own. Model 3 shows the nested model \\(length = 6.042 * female + 4.589\\) where coefficients are estimated from a dataset subsets to only those folk who are not male, and are therefore either nonbinary or female. Naturally, the regression predicts the mean hair length difference between females and nonbinary folk. Note that while the coefficient for female is identical between Model 1 and Model 3, the standard errors are different. Model 4 shows the nested model \\(length = -2.658 * male + 4.589\\) where coefficients are estimated from a dataset subsets to only those folk who are not female, and are therefore either nonbinary or male. Naturally, the regression predicts the mean hair length difference between males and nonbinary folk. Model 5 shows the nested model \\(length = 8.700 * female + 1.931\\) where coefficients are estimated from a dataset subsets to only those folk who are not nonbinary, and are therefore either female or male. Naturally, the regression predicts the mean hair length difference between males and nonbinary folk, and the coefficient (but not standard errors) is identical to the first coefficient in the rotated model using the direction from (0,1) to (1,0). hair length original model rotated model nested: subset x2=0 nested: subset x1=0 nested: subset x1 =1 OR x2 = 1 (1) (2) (3) (4) (5) nonbinary vs female 6.042*** 6.042*** 8.700*** (0.770) (1.050) (0.206) nonbinary vs male -2.658*** -2.658*** (0.770) (0.308) male vs female 8.700*** (0.207) orthogonal to male v female 3.383* (1.526) Constant 4.589*** 6.249*** 4.589*** 4.589*** 1.931*** (0.756) (0.103) (1.031) (0.302) (0.146) Observations 1,630 1,630 830 830 1,600 Adjusted R2 0.520 0.520 0.037 0.082 0.527 F Statistic 884.884*** (df = 2; 1627) 884.884*** (df = 2; 1627) 33.106*** (df = 1; 828) 74.586*** (df = 1; 828) 1,781.628*** (df = 1; 1598) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. The next set of regression results use the second axis system, where the x1-axis represents females, and the x2-axis represents nonbinary folk. The origin therefore represent males. There is a clear symmetry between the first set of regression results and the second. The third axis system, using nonbinary and male for the axes, would produce a similar symmetry. hair length original model rotated model nested: subset x2=0 nested: subset x1=0 nested: subset x1 =1 OR x2 = 1 (1) (2) (3) (4) (5) male vs female 8.700*** 8.700*** 6.042*** (0.207) (0.206) (1.050) male vs nonbinary 2.658*** 2.658*** (0.770) (0.308) nonbinary vs female 6.042*** (0.770) orthogonal to nonbinary vs female 11.358*** (0.824) Constant 1.931*** 6.249*** 1.931*** 1.931*** 4.589*** (0.146) (0.103) (0.146) (0.059) (1.031) Observations 1,630 1,630 1,600 830 830 Adjusted R2 0.520 0.520 0.527 0.082 0.037 F Statistic 884.884*** (df = 2; 1627) 884.884*** (df = 2; 1627) 1,781.628*** (df = 1; 1598) 74.586*** (df = 1; 828) 33.106*** (df = 1; 828) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. 5.2 3D rotated model The next images show the 3D regression surface. The marginal effect of \\(x_1\\), the axis for whether the respondent is female, is shown with a red line. This shows the mean difference between the hair lengths of females and nonbinary folk. The slope of the nested model subsetted to include only nonbinary and female folk is shown with a pink dashed line. Note that the confidence interval is substantially wider than that for the marginal effect. The marginal effect of \\(x_1\\), the axis for whether the respondent is male, is shown with a orange line. The slope of the nested model subsetted to include only nonbinary and male folk is shown with a yellow dashed line. Note that the confidence interval is substantially narrower than that for the marginal effect. The marginal effect of the new direction, the axis that goes between the categories nonbinary and male, is shown with a green line. This has by far the steepest slope. Because it moves between the categories with the most observations, it also has the narrowest confidence interval. The slope of the nested model subsetted to include only male and female folk is shown with a light green dashed line. Although the confidence interval looks almost identical to the marginal effect for the new direction, it is in fact slightly smaller. 5.3 2D visuals "],["adding-explanatory-variables-county-level-voting-behavior-in-2016.html", "Chapter 6 Adding Explanatory Variables: County level voting behavior in 2016 6.1 Directional regression results 6.2 3D regression visualization 6.3 2D rotated model in the direction of x2 predicted by x1", " Chapter 6 Adding Explanatory Variables: County level voting behavior in 2016 This example shows what happens to the regression model when additional explanatory variable are included. Any rotated version of the original model using \\(x_1\\) and \\(x_2\\) will produce identical coefficients and standard errors for the additional explanatory variables as the original model does. In particular, for any new coefficient \\(x_i\\), the coefficient on \\(x_i\\) in the estimated regression surface is interpreted as the marginal effect holding all other variables constant including \\(x_1\\) and \\(x_2\\). The interpretation of the coefficient on \\(x_i\\) remains the same for any model that uses a rotation of \\(x_1\\) and \\(x_2\\). This stands in contrast to nested models, because dropping a variable produces a joint direction between all coefficients and the excluded variable(s), not just the joint change for two variables. That is, rotated models allow you to control for all variables in the original model while providing flexibility in the direction you want to examine \\(x_1\\) and \\(x_2\\). The data in this example shows county level demographics as they relate to the percent of the county that voted for Trump in 2016. The regression being estimated is \\[\\begin{align*} \\text{estimated % voting for Trump} = \\beta_1 * \\text{% with higher education}+ \\beta_2 * \\text{median income} + \\beta_0 \\end{align*}\\] 6.1 Directional regression results The regression results below are generated by stargazer.dira(). Model 1 in the output below shows the results for the original regression. The remaining two models show the results for various rotations on the original model. All model descriptions have been generated automatically by stargazer.dira(). % Trump vote in 2016 original model rotated model rotated model (1) (2) (3) % with higher education -0.462*** (0.033) median income (1,000s) 0.118*** -0.140*** (0.026) (0.020) residuals of % with higher education -0.462*** (0.033) first principal component -0.305*** (0.028) orthogonal to PC 1 0.730*** (0.063) county population -0.00001*** -0.00001*** -0.00001*** (0.00000) (0.00000) (0.00000) increase in drug deaths, 1980-2014 0.002*** 0.002*** 0.002*** (0.0003) (0.0003) (0.0003) Constant 79.082*** 68.208*** 61.287*** (1.474) (1.148) (0.483) Observations 3,139 3,139 3,139 Adjusted R2 0.228 0.228 0.228 F Statistic (df = 4; 3134) 232.987*** 232.987*** 232.987*** Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Model 1: A one unit change in % with higher education, holding median income ($1,000s) constant, is associated with a -0.462 unit change in % Trump vote in 2016. A one unit change in median income ($1,000s), holding % with higher education constant, is associated with a 0.118 unit change in % Trump vote in 2016. Model 2: A one unit change in median income ($1,000s) is associated with a 0.559 unit change in % with higher education AND a -0.14 unit change in % Trump vote in 2016. Including the residuals ensures that the confidence intervals on the rotated regression are identical to the confidence intervals of the original multiple regression. Model 3: The new direction is scaled to be interpreted in units of % with higher education. A one unit change in % with higher education is associated with a 1.327 unit change in median income ($1,000s) AND a -0.305 unit change in % Trump vote in 2016. Note that the coefficients and standard errors on the additional variable, county population and the increase in drug deaths, remain the same across all rotations of the original model. To visualize these rotations, we have to drop back down to the model \\(y \\sim x_1 + x_2\\). Fortunately, the coefficients on \\(x_1\\) and \\(x_2\\) are similar in this model as they were in the model \\(y \\sim x_1 + x_2 + x_3 + x_4\\), as seen below. % Trump vote in 2016 original model rotated model rotated model (1) (2) (3) % with higher education -0.589*** (0.033) median income (1,000s) 0.067* -0.262*** (0.027) (0.020) residuals of % with higher education -0.589*** (0.033) first principal component -0.501*** (0.028) orthogonal to PC 1 0.849*** (0.066) Constant 90.416*** 76.532*** 63.535*** (1.292) (1.029) (0.259) Observations 3,139 3,139 3,139 Adjusted R2 0.134 0.134 0.134 F Statistic (df = 2; 3136) 243.254*** 243.254*** 243.254*** Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Model 1: A one unit change in % with higher education, holding median income ($1,000s) constant, is associated with a -0.589 unit change in % Trump vote in 2016. A one unit change in median income ($1,000s), holding % with higher education constant, is associated with a 0.067 unit change in % Trump vote in 2016. Model 2: A one unit change in median income ($1,000s) is associated with a 0.559 unit change in % with higher education AND a -0.262 unit change in % Trump vote in 2016. Including the residuals ensures that the confidence intervals on the rotated regression are identical to the confidence intervals of the original multiple regression. Model 3: The new direction is scaled to be interpreted in units of % with higher education. A one unit change in % with higher education is associated with a 1.327 unit change in median income ($1,000s) AND a -0.501 unit change in % Trump vote in 2016. 6.2 3D regression visualization 6.3 2D rotated model in the direction of x2 predicted by x1 "],["intersectionality-coming-soon.html", "Chapter 7 Intersectionality (coming soon!)", " Chapter 7 Intersectionality (coming soon!) Directional Regression Analysis can also be used to improve our understanding of intersectionality. Intersectionality, after all, is just a direction in the regression surface. For example, a regression that has gender and race as explanatory variables might want to examine the slope of the regression surface in the direction between (male and black) and (female and white). This package provides the regression results including standard errors for this new direction, as well as visuals to understand it. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
