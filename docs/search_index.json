[["index.html", "Directional Regression Analysis &amp; the DiRA Package Overview", " Directional Regression Analysis &amp; the DiRA Package Ella Foster-Molina 2021-08-16 Overview table, td, th { border: none; padding-left: 1em; padding-right: 1em; margin-left: auto; margin-right: auto; margin-top: 1em; margin-bottom: 1em; } Draft version, not for distribution. Please do not share this link. This document overviews some ways to use the Directional Regression Analysis package. Consider a multiple linear regression surface defined by \\[\\begin{align*} \\hat{y} = \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + ... + \\hat{\\beta}_n x_n + \\hat{\\beta}_0. \\end{align*}\\] The core idea behind the DiRA package is that once you can visualize and interact with a regression surface in 3D, you can productively examine and interpret directions within the regression surface. The DiRA package specifically allows you to: Visualize a regression surface and confidence intervals for a model of the form \\(y \\sim x_1 + x_2\\) or \\(y \\sim x_1 + x_2 + x_1 * x_2\\). Visualize the scatterplot of observations measured by \\(y\\), \\(x_1\\), and \\(x_2\\) along with the regression surface. Visualize regression surfaces estimated from a categorical explanatory variable with three categories. Visualize specifics directions in a regression surface, including The marginal effects and confidence intervals of \\(x_1\\) and \\(x_2\\) in the regression surface. The direction defined by an omitted variable model, which is a type of nested model, \\(y \\sim x_1\\) or \\(y \\sim x_2\\). This direction can show the confidence intervals associated with the nested model. Any rotations of the original model. This is useful for examining the confidence intervals associated with the regression surface for the direction defined by a nested model. It is also useful for examining all possible pairwise comparisons for a 3 level categorical variable, examining intersectionality from 2 categorical variables, or for examining principal component directions. Concisely view the estimated effect and standard errors of the original regression model, as well as any nested or rotated versions of the original regression model. These are the core commands for performing these analyses. Additional details about options available for these commands is available at (put link here). stargazer.dira(formula, data, model.names, data.subsets) Takes a formula, dataset, a vector of model names, and any subsets of the data needed. Additional options are available. Outputs a pretty summary of the rotated and nested models defined in model.names. The first two explanatory variables in the formula are used to create the nests and rotations. models.dira(formula, data, model.names, data.subsets) Takes a formula, dataset, a vector of model names, and any subsets of the data needed. Additional options are available. Creates a named list of rotated and nested linear models. You can examine the models within that list in more depth using tools like glance.dira() and tidy.dira(), or extract individual models to evaluate separately. glance.dira(dira_models) Takes a named list of models created by models.dira() Outputs a tidy overview of each model’s statistics tidy.dira(dira_models) Takes a named list of models created by models.dira() Outputs a tidy overview of the coefficients in each model add_3d.directions(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. Outputs an interactive 3D plot showing the regression surface as well as all specified nested and rotated models. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within add_3d.directions. The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. add_2d.directions(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. Outputs a graphic with 2-4 2D subplots showing the nested models and rotated models projected into 2D plots. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within add_2d.directions(). The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. Note: The graphics object generated by add_2d.directions cannot be modified by the layout command, although plotly object inputted can include a layout layer. The annotate() command can add annotations to the graphics object. Not all models can be plotted in 2D at the moment. plotlist.2d(plotly_object, model.names, data.subsets) Takes a plotly object and a vector of model names, as well as additional options. The command requires x, y, and z variables and a dataset, which can be defined in the main plotly object or within plotlist.2d(). The default regression surface is defined by \\(z \\sim x + y\\), but interaction term models are also accepted. Not all models can be plotted in 2D at the moment. Outputs a list of 2D graphics showing crosssections of the 3D graphic created by add_3d.directions. You can extract individual plots to use as needed. It is currently possible to examine the following nested and rotated models of the multiple linear regression surface. It is also possible to show directional symmetries between the nested and original models for categorical explanatory variables by applying a nested model the original data subsetted to only include one or two of the categories. model model name direction of first axis direction of second axis line color(s) y ~ x1 + x2 lm_multiple x1 x2 red &amp; orange y ~ x1 lm_y.predby.x1 x2 predicted by x1 light blue y ~ x1 + x2 residuals lm_y.predby.x1.w.resids x2 predicted by x1 residuals of x2 dark blue &amp; blue y ~ x2 lm_y.predby.x2 x1 predicted by x2 fuchsia y ~ x2 + x1 residuals lm_y.predby.x2.w.resids x1 predicted by x2 residuals of x1 purple &amp; pink y ~ PC1 lm_pc first principal component light grey y ~ PC1 + PC2 lm_pc.w.orth first principal component orthogonal to first axis black &amp; grey y ~ new.dir + orth lm_new.direction user defined direction orthogonal to first axis green &amp; lime green "],["MultipleRegression.html", "EXAMPLE 1 Multiple Regression: County level voting behavior in 2016 1.1 Regression results for the original and nested models 1.2 Regression visualization in 3D 1.3 Examining the nested model in 2D", " EXAMPLE 1 Multiple Regression: County level voting behavior in 2016 The primary functionality of the DiRA package is to show regression surfaces in 3 dimensions. This allows you to Better understand marginal effects by visualizing what it means to hold other covariates in the model constant. Visualize the direction of a nested model (also called an omitted variable model) in the 3 dimensional regression surface. See the regression results for the regression surface and the nested model in a convenient form. The data used in this example shows county level demographics as they relate to the percent of the county that voted for Trump in 2016. The variables used in the multiple regression model are as follows. Outcome variable: The percent of the county’s voters that voted for Donald Trump in 2016. Explanatory variable: The percent of the county that has completed some higher education, regardless of whether they completed a higher education degree. Explanatory variable: The log of the percent of the county that is unemployed. This example will start with a simple linear regression, the connect that simple linear regression to a multiple regression. The equation for the simple linear regression is \\[\\begin{align*} \\hat{y} &amp;= \\hat{\\beta}_1 x_1 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} &amp;= -0.534 * \\text{% with higher education} + 90.913.\\\\ \\end{align*}\\] The corresponding scatterplot and regression line are visualized below. There is a clear negative relationship between the two variables. Increasing higher education experience in a county is associated with lower support for Trump in the 2016 election. However, this only examines one possible relationship in the data. Unemployment would also be expected to have a relationship with the Trump vote. A multiple regression, which predicts a regression surface, can examine how both county level unemployment and education relate to support for Trump. This regression takes the form \\[\\begin{align*} \\hat{y} &amp;= \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} = &amp; -0.781 * \\text{% with higher education} - \\\\ &amp; 16.727 * \\text{logged % unemployed} + 130.274. \\end{align*}\\] 1.1 Regression results for the original and nested models The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression results associated with the original multiple linear regression. The regression results for the nested model \\[\\text{estimated % Trump vote} = -0.534 * \\text{% with higher education} + 90.913\\] are in Model 2. % Trump vote in 2016 original model nested model (1) (2) % with higher education -0.781*** -0.534*** (0.026) (0.024) logged % unemployed -16.727*** (0.817) Constant 130.274*** 90.913*** (2.265) (1.277) Observations 3,139 3,139 Adjusted R2 0.234 0.132 F Statistic 481.583*** (df = 2; 3136) 479.755*** (df = 1; 3137) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 1.2 Regression visualization in 3D The image below is created using the add_3d.directions() command in the DiRA package. It shows: A scatterplot of counties measured by the percent of their population with higher education, the logged percent unemployment in each county, and the percent of the county that voted for Trump in 2016. The regression surface defined by Model 1. The marginal effect of each of the variables within Model 1 and Model 2. Confidence intervals associated with the regression surface and the marginal effects of the variables in Model 1 and Model 2. To interact with the 3D visual: Hover over points to see the county name and state. Click and drag on the graphic to rotate it. Click on items in the legend to remove specific parts of the image; click on that item again to put it back. To reset the image to its starting point, hover over the image and click on the house symbol (or reload the page). Model 1, \\(y \\sim x_1 + x_2\\), is shown with regression surface and a confidence interval surface. The marginal effects are shown in red and orange lines. Hover over and spin the graphic to examine the effects described below. Since this section focuses on Model 1, you may want to remove the dashed light blue line representing Model 2 for the moment. The marginal effect of \\(x_1\\) (% with higher education) in red, shows how much y (% Trump) changes when \\(x_2\\) (log % unemployed) is held constant. In this model, the effect of % with higher education is strongly negative. Holding unemployment constant, counties with low education levels (~30% with higher education) tend to have over 80% of their voters vote for Trump. Those with high education levels (~80% with higher education) tend to have less than 40% of their voters vote for Trump. The marginal effect of \\(x_2\\) (log % unemployed) in orange, shows how much y (% Trump) changes when \\(x_1\\) (% with higher education) is held constant. In this model, the effect of % with higher education is also strongly negative holding education levels constant. That is, counties with higher unemployment tend to have lower rates of voting for Trump in 2016 when county education levels are controlled for. In this visualization, the marginal effect of \\(x_1\\) (\\(x_2\\)) is shown holding \\(x_2\\) (\\(x_1\\)) at its mean. The DiRA package defaults to holding all other covariates at their means, but any value could be chosen. The y-intercept for the marginal effect would change, but the slope would remain the same regardless of the value chosen. 1.3 Examining the nested model in 2D The image above is created using the add_2d.directions() command in the DiRA package. Model 2, \\(y \\sim x_1\\) is a nested model of Model 1 and only has one axis. This axis is shown with a light blue dashed line, and it has a direction in the \\(x_1\\), \\(x_2\\) coordinate system defined by the best fit line for \\(x_2 \\sim x_1\\), \\[\\text{logged % unemployed} = -0.534 * \\text{% with higher education} + 90.913.\\] The intent of these 2D graphics are to show that the simple linear regression defines a direction in the multiple regression surface. By omitting \\(x_2\\) from the model, we end up modeling the change in \\(y\\) produced by a joint direction between \\(x_1\\) and \\(x_2\\) in the regression surface. You can (and should!) spin the 3D graphic to reflect the 2D images. The images are not identical because the 3D image has a perspective shift, but you can see that the 2D images are simply crossections of the 3D graphic. Tips to improve the view: Focus on the light blue dashed line that represents Model 2 in the 3D graphic by removing the red and orange lines (click on the corresponding items in the legend). Click and drag from the top of the 3D plot to the bottom to see the top of the regression surface. Hover over individual scatterpoints in any of these images to see which county and state the point represents. "],["interaction-terms-county-level-voting-behavior-in-2016.html", "EXAMPLE 2 Interaction Terms: County level voting behavior in 2016 2.1 Regression results for a model with interaction terms 2.2 3D changing marginal effects 2.3 2D marginal effects", " EXAMPLE 2 Interaction Terms: County level voting behavior in 2016 Interaction terms in multiple regressions introduce an element of nonlinearity to the regression surface. This nonlinearity can make the model difficult to interpret. The visualizations available in the DiRA package are designed to help facilitate interpretation. This example highlights a regression surface estimated from a model with an interaction term where the slope of the marginal effects not only change substantially based on the value of the other covariate in the model, but also flip signs. The regression examines the relationship between: Outcome variable: The percent of the county’s voters that voted for Donald Trump in 2016. Explanatory variable: The logged opioid overdosed death rate in 2014, where the original opioid overdose death rate is measured in deaths per 100,000 people. Explanatory variable: A binary variable that is true/1 if a majority of the adults in the county had attended college in 2016, whether or not they attained a degree. It is false/0 otherwise. This variable is binary for ease of interpretation and to show the relationship between the regression surface and the nested models applied to subsets of the data. 2.1 Regression results for a model with interaction terms Consider the following estimated regression surface, \\[\\begin{align*} \\hat{y} =&amp; \\hat{\\beta}_1 x_1 +\\hat{\\beta}_2 x_2 +\\hat{\\beta}_{12} x_1 x_2 + \\hat{\\beta}_0 \\\\ \\text{estimated % Trump vote} =&amp; - 6.652 * \\text{opioid death rate (log)} + \\\\ &amp; 19.832 * \\text{majority attended college} \\\\ &amp; -13.056 * \\text{opioid death rate (log)}* \\text{majority attended college}+ 52.753 \\\\ \\end{align*}\\] The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression results associated with the original multiple linear regression. Model 2 shows the regression results for the nested model using only the subset of the data with low opioid overdose deaths (\\(x_2 = 0\\)), while Model 3 shows the results where opioid overdose deaths are high. All coefficients are statistically significant. However, the original model is difficult to interpret directly. It is easy but erroneous to interpret the logged opioid death rate coefficient of 6.652 in Model 1 as positive regardless of the value of the college attendance variable. However, the logged opioid death rate coefficient is only positive and 6.652 when the \\(x_2\\) variable is zero, which is when the majority of the county did not attend college. When the majority of the county does attend college, the marginal effect of the logged opioid death rate becomes strongly negative. This is evident in the results from Models 2 and 3, where the logged opioid death rate flips direction between low and high education counties. The interaction term shows how much the marginal effect of \\(x_1\\) changes as \\(x_2\\) changes, and vice versa. It also shows whether that change is statistically significant. In this case it is statistically significant. That is, we believe that the difference in the slopes of the \\(x_1\\) as \\(x_2\\) changes are not due to chance. This is one of the key benefits of using an interaction term instead of running a nested regression on a subset of the data. As you’ll see in the 3D visualization, the marginal effect of the logged opioid death rate in the original model when \\(x_2 = 0\\) is identical to the coefficient on the logged opioid death rate in Model 2. You’ll also see that the standard errors and confidence intervals are slightly different. % Trump vote in 2016 original model nested: subset x2=0 nested: subset x2=1 (1) (2) (3) opioid death rate (log) 6.652*** 6.652*** -6.404*** (0.714) (0.652) (0.711) majority attended college 19.832*** (2.179) opioid death rate (log):majority attended college -13.056*** (0.975) Constant 52.753*** 52.753*** 72.585*** (1.681) (1.535) (1.482) Observations 3,139 1,456 1,683 Adjusted R2 0.120 0.066 0.046 F Statistic 144.219*** (df = 3; 3135) 104.008*** (df = 1; 1454) 81.209*** (df = 1; 1681) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 2.2 3D changing marginal effects The visual below shows the marginal effect of the logged rate of opioid overdose deaths (\\(x_1\\)) with two red lines. One line shows the marginal effect for counties where a majority attended college and the other for counties where a majority did not attend college. Unlike the regression example above without an interaction effect, the marginal effects have very different slopes based on college experience. The nested models are shown with pink dashed lines. The predicted values of the nested and original models are identical for counties with high education or low education, so the pink dashed line lies on top of the red line. The confidence intervals, however, are slightly different between the marginal effects in the nested and original models. If you zoom in far enough, you’d see that the confidence interval on the logged opioid death rate is slightly wider for high education counties. It is slightly narrower for counties with low rates of college attendance. To see how much the marginal effect of one variable change based on the value of another in your own data, you can: Use the DiRA package (currently available by asking the author, Ella Foster-Molina, for a copy). Run the regression \\(y \\sim x_1\\) on different values or subsets of \\(x_2\\) (or vice versa). Calculate the marginal effect of \\(x_1\\) from the estimated regression surface equation, plugging in new values of \\(x_2\\) (or vice versa). 2.3 2D marginal effects One benefit of being able to visualize the marginal effects and confidence intervals of the original model for different values of the college attendance variable is that you can see whether the marginal effect itself is statistically significantly positive/negative for any given value of college attendance. There is undoubtedly a way to calculate the statistical significance of the marginal effect of \\(x_1\\) for a given value of \\(x_2\\), but it would be complex (is it? I don’t know how to do it off the top of my head). Each of the marginal effects plotted above is statistically significantly different from zero. "],["CategoricalVar.html", "EXAMPLE 3 Categorical Data &amp; Rotated Models: Simulated hair data 3.1 Understanding categorical variables 3.2 Directional regression results 3.3 3D rotated model 3.4 2D visuals", " EXAMPLE 3 Categorical Data &amp; Rotated Models: Simulated hair data Directional regression analysis can allow the comparison between all possible pairwise comparisons of a categorical variable using a single regression surface. Although the principal can be applied to as variables with as many categories as needed, the DiRA package currently only allows for a direct analysis of categorical variables with 3 or fewer categories. This example uses a simulated dataset that shows a relationship between gender identification and hair length. The gender categories used in this simulation are male, female, and nonbinary. 3.1 Understanding categorical variables To understand the rotations and subsets that are created in the models, it’s first important to understand how categorical variables are represented in a regression. Each category within the categorical variable represents its own axis. Each category’s axis has FALSE/0 represent those observations that are not part of that category, and TRUE/1 represent those observations that are part of the category. For example, the female axis has 1 indicate females, and 0 indicate not females. Programs typically code 0 as FALSE, and 1 as TRUE. However, every new axis must provide new information in the model. You can see in the image below that a coordinate system using female and male as the axes uniquely identifies the nonbinary folk at the origin of the plot; they are the only group that is both not female and not male, because there are only 3 categories available. An additional axis to represent the category that is at the origin would not provide new information, and therefore is not allowed in a regression. Some programs (like R) will simply drop redundant variables if you try to include them in a regression, while others will throw errors. One result of this axis system is that only two comparisons are typically visible for the gender variable, because regression results typically only show marginal effects. The marginal effects for the axis system shown above would reflect the hair length difference between nonbinary folk and females, and between nonbinary folk and males. Neither marginal effect shows the difference between females and males, even though that difference is shown in the regression surface. To uncover it, we have to examine a direction in the regression surface, namely the direction from the point representing males, which is (0,1) = (not female, male), to the point representing females, which is (1,0) = (female, not male). This direction will produce both a slope that represents the difference in hair lengths between males and female, as well as a confidence interval. Computing this direction requires rotating the original \\(x_1\\) and \\(x_2\\) axes to reflect the new direction, then calculating marginal effects for the new rotated axes. The DiRA package does this for you, as shown below. 3.2 Directional regression results The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression \\(length = 6.042 * female - 2.658* male + 4.589\\). The marginal effect of each variable shows the mean difference between the reference category and the category indicated by the variable. Since the reference category is nonbinary folk, females tend to have hair lengths 6.042 inches longer than nonbinary folk. Men tend to have hair lengths 2.658 inches shorter than nonbinary folk. Model 2 shows the rotation of the female and male axes that represents the direct comparison between males and females. This is done by telling stargazer.dira() to show the results for a rotation of the original axes where the primary axis moves from the starting point (not female, male) = (0, 1) to the ending point (female, not male) = (1,0). This rotation shows that females tend to have hair lengths 8.7 inches longer than men. There is also an orthogonal component included; without it, the male vs female direction would be a joint direction from male to a combination of female and male instead of the difference between males and female holding the nonbinary variable constant. The orthogonal component has no meaningful interpretation on its own. hair length original model rotated model (1) (2) nonbinary vs female 6.042*** (0.770) nonbinary vs male -2.658*** (0.770) male vs female 8.700*** (0.207) orthogonal to male v female 3.383* (1.526) Constant 4.589*** 6.249*** (0.756) (0.103) Observations 1,630 1,630 Adjusted R2 0.520 0.520 F Statistic (df = 2; 1627) 884.884*** 884.884*** Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. 3.3 3D rotated model The next images show the 3D regression surface. The marginal effect of \\(x_1\\), the axis for whether the respondent is female, is shown with a red line. This shows the mean difference between the hair lengths of females and nonbinary folk. The marginal effect of \\(x_1\\), the axis for whether the respondent is male, is shown with a orange line. The marginal effect of the new direction, the axis that goes between the categories nonbinary and male, is shown with a green line. This has by far the steepest slope. Because it moves between the categories with the most observations, it also has the narrowest confidence interval. The marginal effect of axis orthogonal to the new direction is shown with a light green line. This line controls for the effect of nonbinary folk, but has no substantive interpretation on its own. It ensures that the marginal effect of the new direction accurately reflects the difference between males and females, instead of the difference between the hair length for a combination of nonbinary folk and females to the hair length of males. 3.4 2D visuals The two dimensional visuals above show three crossections of the regression surface plotted in 3D. A deeper dive into understanding categorical explanatory variables is available in the optional Examples 6 and 7: Example 6 shows how the rotated model shown above can be replicated by changing the reference category. Example 7 shows how nested models applied to a subset of the data can reproduce the marginal effects shown above. Although the marginal effects can be replicated using nested models, the confidence intervals are not the same. (Note for later: One unanswered question regards the shape of the confidence intervals. The confidence interval shape for the coefficient on \\(x_1\\) = isFemale_num clearly does not remain constant as \\(x_2\\) = isMale_num changes. However, there is only one standard error associated with the marginal effect of \\(x_1\\). Is this because the standard error represents some kind of average of the standard errors? Is the average error across the range of \\(x_1\\) the same for all values of \\(x_2\\), even as the error at any given point of \\(x_1\\) changes? ) "],["intersectionality-coming-soon.html", "EXAMPLE 4 Intersectionality (coming soon!)", " EXAMPLE 4 Intersectionality (coming soon!) Directional Regression Analysis can also be used to improve our understanding of intersectionality. Intersectionality, after all, is just a direction in the regression surface. For example, a regression that has gender and race as explanatory variables might want to examine the slope of the regression surface in the direction between (male and black) and (female and white). This package provides the regression results including standard errors for this new direction, as well as visuals to understand it. "],["adding-explanatory-variables-coming-soon.html", "EXAMPLE 5 Adding Explanatory Variables (coming soon!)", " EXAMPLE 5 Adding Explanatory Variables (coming soon!) This example shows what happens to the regression model when additional explanatory variable are included. Using rotations of the original model allows you to replicate the coefficients from a nested model. There are two benefits to using rotations instead of dropping variables to create nested models. The standard error for the marginal effect of the rotated axes tends to be more precise than the standard error attached to that axis on a nested model. Using the rotation allows you to control for all variables in the model instead of “biasing” them by creating a joint direction between those omitted variable(s) and the included variables. In particular, for any new coefficient \\(x_i\\), the coefficient on \\(x_i\\) in the estimated regression surface is interpreted as the marginal effect holding all other variables constant including \\(x_1\\) and \\(x_2\\). The interpretation of the coefficient on \\(x_i\\) remains the same for any model that uses a rotation of \\(x_1\\) and \\(x_2\\). This stands in contrast to nested models, because dropping a variable produces a joint direction between all coefficients and the excluded variable(s), not just the joint change for two variables. That is, nested models give you flexibility in the direction you want to examine the changes in \\(x_1\\) and \\(x_2\\). Rotated models allow the same flexibility and more, while still controlling for all variables in the original model. The data in this example shows… "],["categoricalRotationSymmetries.html", "EXAMPLE 6 (Optional) Categorical Data &amp; Symmetries in Rotated models: Simulated hair data 6.1 Directional regression results for different reference categories 6.2 3D rotated models", " EXAMPLE 6 (Optional) Categorical Data &amp; Symmetries in Rotated models: Simulated hair data Example 3 showed how rotations in a regression model can allow you to show the change in the outcome variable in new directions. This is a deeper dive into that example, showing that the rotations can be reproduced by changing the reference category when working with categorical explanatory variables. Since the DiRA package currently requires that categorical variables be turned into a series of dummy/indicator variables, to change the reference category you will simply change which dummy/indicator variables are used in the regression model. The left hand image shows the representation of an explanatory variable when the reference category is nonbinary folk, and the right hand image shows the representation when the reference category is males. 6.1 Directional regression results for different reference categories The first set of regression results are the same as the prior example, where the x1-axis is represented with the female category, and the x2-axis is represented with the male category. The reference category is nonbinary folk. hair length original model rotated model (1) (2) nonbinary vs female 6.042*** (0.770) nonbinary vs male -2.658*** (0.770) male vs female 8.700*** (0.207) orthogonal to male v female 3.383* (1.526) Constant 4.589*** 6.249*** (0.756) (0.103) Observations 1,630 1,630 Adjusted R2 0.520 0.520 F Statistic (df = 2; 1627) 884.884*** 884.884*** Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. The next set of regression results use the second axis system, where the x1-axis represents females, and the x2-axis represents nonbinary folk. The origin/reference category therefore represent males. There is a clear symmetry between the first set of regression results and the second. The third axis system, using nonbinary and male for the axes, would produce a similar symmetry. hair length original model rotated model (1) (2) male vs female 8.700*** (0.207) male vs nonbinary 2.658*** (0.770) nonbinary vs female 6.042*** (0.770) orthogonal to nonbinary vs female 11.358*** (0.824) Constant 1.931*** 6.249*** (0.146) (0.103) Observations 1,630 1,630 Adjusted R2 0.520 0.520 F Statistic (df = 2; 1627) 884.884*** 884.884*** Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. 6.2 3D rotated models The next images show the 3D regression surfaces associated with each of the two coordinate systems. The slopes of the marginal effects associated with each pairwise comparison in the gender variable are identical between the two regression surfaces. "],["categoricalNestedModels.html", "EXAMPLE 7 (Optional) Subsetted Categorical Explanatory Variables &amp; Nested Models: Simulated hair data 7.1 Directional regression results 7.2 3D rotated model 7.3 2D visuals", " EXAMPLE 7 (Optional) Subsetted Categorical Explanatory Variables &amp; Nested Models: Simulated hair data Example 3 and Example 7 began the exploration of into categorical explanatory variables. The regression results already shown describe the differences in mean hair lengths between each of the pairwise comparisons in the categorical variable. This example continues that exploration. by showing that these differences can also be recovered using a nested model applied to a subset of the data. However, although the differences are identical between the regression surface and the nested models, the confidence intervals are not. 7.1 Directional regression results As before, Model 1 shows the regression \\(length = 6.042 * female - 2.658* male + 4.589\\). The marginal effect of each variable shows the mean difference between the reference category and the category indicated by the variable. Since the reference category is nonbinary folk, females tend to have hair lengths 6.042 inches longer than nonbinary folk. Men tend to have hair lengths 2.658 inches shorter than nonbinary folk. As before, Model 2 shows the rotation of the female and male axes that represents the direct comparison between males and females. This is done by telling stargazer.dira() to show the results for a rotation of the original axes where the primary axis moves from the starting point (not female, male) = (0, 1) to the ending point (female, not male) = (1,0). This rotation shows that females tend to have hair lengths 8.7 inches longer than men. There is also an orthogonal component included; without it, the male vs female direction would be a joint direction from male to a combination of female and male instead of the difference between males and female holding the nonbinary variable constant. The orthogonal component has no meaningful interpretation on its own. Model 3 shows the nested model \\(length = 6.042 * female + 4.589\\) where coefficients are estimated from a dataset subsets to only those folk who are not male, and are therefore either nonbinary or female. Naturally, the regression predicts the mean hair length difference between females and nonbinary folk. Note that while the coefficient for female is identical between Model 1 and Model 3, the standard errors are different. Model 4 shows the nested model \\(length = -2.658 * male + 4.589\\) where coefficients are estimated from a dataset subsets to only those folk who are not female, and are therefore either nonbinary or male. Naturally, the regression predicts the mean hair length difference between males and nonbinary folk. Model 5 shows the nested model \\(length = 8.700 * female + 1.931\\) where coefficients are estimated from a dataset subsets to only those folk who are not nonbinary, and are therefore either female or male. Naturally, the regression predicts the mean hair length difference between males and nonbinary folk, and the coefficient (but not standard errors) is identical to the first coefficient in the rotated model using the direction from (0,1) to (1,0). hair length original model rotated model nested: subset x2=0 nested: subset x1=0 nested: subset x1 =1 OR x2 = 1 (1) (2) (3) (4) (5) nonbinary vs female 6.042*** 6.042*** 8.700*** (0.770) (1.050) (0.206) nonbinary vs male -2.658*** -2.658*** (0.770) (0.308) male vs female 8.700*** (0.207) orthogonal to male v female 3.383* (1.526) Constant 4.589*** 6.249*** 4.589*** 4.589*** 1.931*** (0.756) (0.103) (1.031) (0.302) (0.146) Observations 1,630 1,630 830 830 1,600 Adjusted R2 0.520 0.520 0.037 0.082 0.527 F Statistic 884.884*** (df = 2; 1627) 884.884*** (df = 2; 1627) 33.106*** (df = 1; 828) 74.586*** (df = 1; 828) 1,781.628*** (df = 1; 1598) Note: p&lt;0.05; p&lt;0.01; p&lt;0.001 Note: The coefficients for y ~ new.direction + new.dir.orth have been scaled to be interpreted in units of x1. 7.2 3D rotated model The next images show the 3D regression surface. The marginal effect of \\(x_1\\), the axis for whether the respondent is female, is shown with a red line. This shows the mean difference between the hair lengths of females and nonbinary folk. The slope of the nested model subsetted to include only nonbinary and female folk is shown with a pink dashed line. Note that the confidence interval based on the nested model is wider than that for the marginal effect based on the regression surface. The marginal effect of \\(x_1\\), the axis for whether the respondent is male, is shown with a orange line. The slope of the nested model subsetted to include only nonbinary and male folk is shown with a yellow dashed line. Note that the confidence interval based on the nested model is substantially narrower than that for the marginal effect based on the regression surface. The marginal effect of the new direction, the axis that goes between the categories nonbinary and male, is shown with a green line. This has by far the steepest slope. Because it moves between the categories with the most observations, it also has the narrowest confidence interval. The slope of the nested model subsetted to include only male and female folk is shown with a light green dashed line. Although the confidence interval from the nested model looks almost identical to the marginal effect for the new direction, it is in fact slightly smaller. 7.3 2D visuals The 2 dimensional graphics above highlight the difference between the confidence intervals in the nested models as compared to the confidence intervals in the regression surface along the direction that produces the relevant pairwise comparison between categories in the gender variable. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
