# Interaction Terms: County level voting behavior in 2016

Interaction terms in multiple regressions introduce an element of nonlinearity to the regression surface. This nonlinearity can make the model difficult to interpret. The visualizations available in the DiRA package are designed to help facilitate interpretation. 

This example highlights a regression surface estimated from a model with an interaction term where the slope of the marginal effects not only change substantially based on the value of the other covariate in the model, but also flip signs. The regression examines the relationship between: 

  - **Outcome variable**: The percent of the county's voters that voted for Donald Trump in 2016. 
  - **Explanatory variable**: The logged opioid overdosed death rate in 2014, where the original opioid overdose death rate is measured in deaths per 100,000 people. 
  - **Explanatory variable**: A binary variable that is true/1 if a majority of the adults in the county had attended college in 2016, whether or not they attained a degree. It is false/0 otherwise. This variable is binary for ease of interpretation and to show the relationship between the regression surface and the nested models applied to subsets of the data.

## Regression results for a model with interaction terms

Consider the following estimated regression surface, 

\begin{align*}
\hat{y} =& \hat{\beta}_1 x_1 +\hat{\beta}_2 x_2 +\hat{\beta}_{12} x_1 x_2 +  \hat{\beta}_0 \\
\text{estimated % Trump vote} =&  - 6.652 * \text{opioid death rate (log)} + \\
              & 19.832 * \text{majority attended college} \\
                &  -13.056 * \text{opioid death rate (log)}* \text{majority attended college}+ 52.753 \\
\end{align*}


The regression results for this model are created using the stargazer.dira() command in the DiRA package. Model 1 shows the regression results associated with the original multiple linear regression. Model 2 shows the regression results for the nested model using only the subset of the data with low opioid overdose deaths ($x_2 = 0$), while Model 3 shows the results where opioid overdose deaths are high.

All coefficients are statistically significant. However, the original model is difficult to interpret directly. It is easy but erroneous to interpret the logged opioid death rate coefficient of 6.652 in Model 1 as positive regardless of the value of the college attendance variable. However, the logged opioid death rate coefficient is only positive and 6.652 when the $x_2$ variable is zero, which is when the majority of the county did not attend college. When the majority of the county does attend college, the marginal effect of the logged opioid death rate becomes strongly negative. This is evident in the results from Models 2 and 3, where the logged opioid death rate flips direction between low and high education counties.

The interaction term shows how much the marginal effect of $x_1$ changes as $x_2$ changes, and vice versa. It also shows whether that change is statistically significant. In this case it is statistically significant. That is, we believe that the difference in the slopes of the $x_1$ as $x_2$ changes are not due to chance. This is one of the key benefits of using an interaction term instead of running a nested regression on a subset of the data. 

As you'll see in the 3D visualization, the marginal effect of the logged opioid death rate in the original model when $x_2 = 0$ is identical to the coefficient on the logged opioid death rate in Model 2. You'll also see that the standard errors and confidence intervals are slightly different.

```{r, results = 'asis'}
model.names <-  c("lm_multiple", "lm_y.predby.x1.subset.x2.0", "lm_y.predby.x1.subset.x2.1")

stargazer.dira(formula = prcntGOP16~ drug2014.log * highCollege_num, data = countyData,
               model.names = model.names, units.of.x1 = T , print.model.descriptions = F,
               type = type,  keep.stat = c("n","aic", "adj.rsq", "f"),
               star.cutoffs = c(0.05, 0.01, 0.001),
               covariate.labels = c("opioid death rate (log)", "majority attended college",
                                    "opioid death rate (log):majority attended college",
                                    "Constant"),
               dep.var.caption = "% Trump vote in 2016"
               )
```

## 3D changing marginal effects

The visual below shows the marginal effect of the logged rate of opioid overdose deaths ($x_1$) with two <span style="color: crimson;">red</span> lines. One line shows the marginal effect for counties where a majority attended college and the other for counties where a majority did not attend college. Unlike the regression example above without an interaction effect, the marginal effects have very different slopes based on college experience. 

The nested models are shown with <span style="color: violet;">pink</span> dashed lines. The predicted values of the nested and original models are identical for counties with high education or low education, so the pink dashed line lies on top of the red line. The confidence intervals, however, are slightly different between the marginal effects in the nested and original models. If you zoom in far enough, you'd see that the confidence interval on the logged opioid death rate is slightly wider for high education counties. It is slightly narrower for counties with low rates of college attendance.

To see how much the marginal effect of one variable change based on the value of another in your own data, you can:

  - Use the DiRA package (currently available by asking the author, Ella Foster-Molina, for a copy).
  - Run the regression $y \sim x_1$ on different values or subsets of $x_2$ (or vice versa).
  - Calculate the marginal effect of $x_1$ from the estimated regression surface equation, plugging in new values of $x_2$ (or vice versa).

```{r }
plot_ly(data = countyData, 
              x = ~drug2014.log,
              y = ~highCollege_num,
              z = ~prcntGOP16 ,text = ~countyState, 
              color = ~highCollege,
              colors = c("deepskyblue", "dimgrey")) %>%
  add_markers(data = countyData, size = 1) %>% 
  add_marginals(formula =prcntGOP16~ drug2014.log * highCollege_num, plot.surface = F, ci = T,
                          define.constant.x1 = 3, define.constant.x2 = 1) %>%
  add_3d.directions(formula =prcntGOP16~ drug2014.log * highCollege_num, model.names = model.names, 
                          define.constant.x1 = 1, define.constant.x2 = 0, ci =T) %>%
  layout( 
    title = "\nCounty Vote for Trump by\n education & income",
    scene = list(xaxis = list(title = 'logged opioid overdose death rate'),
                 yaxis = list(title = 'education'),
                 zaxis = list(title = '% vote for Trump 2016')),
    # width = 500, height = 500,
    legend = list(font = list(size = 8))
    )
```

## 2D marginal effects

```{r fig.height= 4.5, fig.width = 7.5}
p <- plot_ly(data = countyData, 
              x = ~drug2014.log,
              y = ~highCollege_num,
              z = ~prcntGOP16,
             color = ~ highCollege,
              colors = c("deepskyblue", "dimgrey"),
              text = ~countyState ) %>%
  add_markers(
              # color = ~prcntGOP16,
              # colors = c("black", "orange"),
              name = "scatterplot of counties") %>%
  layout( 
    title = "\nCounty Vote for Trump by\n education & income",
    scene = list(xaxis = list(title = 'logged opioid overdose death rate'),
                 yaxis = list(title = 'education'),
                 zaxis = list(title = '% vote for Trump 2016')),
    # width = 500, height = 500,
    legend = list(font = list(size = 8))
    )

p %>% add_2d.directions(formula =prcntGOP16~ drug2014.log * highCollege_num, model.names)%>% 
      add_annotations(
        text = 'majority did not attend college', x = 0.1, y = 1, yref = "paper",
        xref = "paper", xanchor = "middle", yanchor = "top", showarrow = FALSE, font = list(size = 15)
      ) %>% 
      add_annotations(
        text = 'majority attended college', x = 0.9, y = 1, yref = "paper",
        xref = "paper", xanchor = "middle", yanchor = "top", showarrow = FALSE, font = list(size = 15)
      )


```

One benefit of being able to visualize the marginal effects and confidence intervals of the original model for different values of the college attendance variable is that you can see whether the marginal effect itself is statistically significantly positive/negative for any given value of college attendance. There is undoubtedly a way to calculate the statistical significance of the marginal effect of $x_1$ for a given value of $x_2$, but it would be complex (is it? I don't know how to do it off the top of my head). Each of the marginal effects plotted above is statistically significantly different from zero.
